#
# Copyright (c) 2021 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import copy
import math
import os
import warnings
from typing import Optional, Tuple, Union

import torch
from torch import nn

from transformers.pytorch_utils import prune_linear_layer, find_pruneable_heads_and_indices
from transformers.models.t5.configuration_t5 import T5Config
import intel_extension_for_pytorch as ipex

class T5Attention(nn.Module):
    def __init__(self, config: T5Config, has_relative_attention_bias=False):
        super().__init__()
        self.is_decoder = config.is_decoder
        self.has_relative_attention_bias = has_relative_attention_bias
        self.relative_attention_num_buckets = config.relative_attention_num_buckets
        self.relative_attention_max_distance = config.relative_attention_max_distance
        self.d_model = config.d_model
        self.key_value_proj_dim = config.d_kv
        self.n_heads = config.num_heads
        self.dropout = config.dropout_rate
        self.inner_dim = self.n_heads * self.key_value_proj_dim

        # Mesh TensorFlow initialization to avoid scaling before softmax
        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)
        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)
        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)
        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)
        self.qkv =  nn.Linear(self.inner_dim, self.d_model*3, bias=False)

        if self.has_relative_attention_bias:
            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)
        self.pruned_heads = set()
        self.gradient_checkpointing = False

    def prune_heads(self, heads):
        if len(heads) == 0:
            return
        heads, index = find_pruneable_heads_and_indices(
            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads
        )
        # Prune linear layers
        self.q = prune_linear_layer(self.q, index)
        self.k = prune_linear_layer(self.k, index)
        self.v = prune_linear_layer(self.v, index)
        self.qkv = prune_linear_layer(self.v, index*3)
        self.o = prune_linear_layer(self.o, index, dim=1)
        # Update hyper params
        self.n_heads = self.n_heads - len(heads)
        self.inner_dim = self.key_value_proj_dim * self.n_heads
        self.pruned_heads = self.pruned_heads.union(heads)

    @staticmethod
    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):
        """
        Adapted from Mesh Tensorflow:
        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593
        Translate relative position to a bucket number for relative attention. The relative position is defined as
        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to
        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for
        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative
        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.
        This should allow for more graceful generalization to longer sequences than the model has been trained on
        Args:
            relative_position: an int32 Tensor
            bidirectional: a boolean - whether the attention is bidirectional
            num_buckets: an integer
            max_distance: an integer
        Returns:
            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)
        """
        relative_buckets = 0
        if bidirectional:
            num_buckets //= 2
            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets
            relative_position = torch.abs(relative_position)
        else:
            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))
        # now relative_position is in the range [0, inf)

        # half of the buckets are for exact increments in positions
        max_exact = num_buckets // 2
        is_small = relative_position < max_exact

        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
        relative_postion_if_large = max_exact + (
            torch.log(relative_position.float() / max_exact)
            / math.log(max_distance / max_exact)
            * (num_buckets - max_exact)
        ).to(torch.long)
        relative_postion_if_large = torch.min(
            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)
        )

        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)
        return relative_buckets

    def compute_bias(self, query_length, key_length, device=None):
        """Compute binned relative position bias"""
        if device is None:
            device = self.relative_attention_bias.weight.device
        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
        relative_position = memory_position - context_position  # shape (query_length, key_length)
        relative_position_bucket = self._relative_position_bucket(
            relative_position,  # shape (query_length, key_length)
            bidirectional=(not self.is_decoder),
            num_buckets=self.relative_attention_num_buckets,
            max_distance=self.relative_attention_max_distance,
        )
        values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)
        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)
        return values

    def forward(
        self,
        hidden_states,
        mask=None,
        key_value_states=None,
        position_bias=None,
        past_key_value=None,
        layer_head_mask=None,
        query_length=None,
        use_cache=False,
        output_attentions=False,
    ):
        """
        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).
        """
        # Input is (batch_size, seq_length, dim)
        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)
        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)
        batch_size, seq_length = hidden_states.shape[:2]

        real_seq_length = seq_length

        if past_key_value is not None:
            assert (
                len(past_key_value) == 2
            ), f"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states"
            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length

        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]

        def shape(states):
            """projection"""
            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)

        def unshape(states):
            """reshape"""
            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)

        if key_value_states is None:
            output = self.qkv(hidden_states)
            query_states = shape(output[:, :, 0: self.inner_dim])
            key_states = shape(output[:, :,  self.inner_dim :  self.inner_dim * 2])
            value_states = shape(output[:, :,  self.inner_dim * 2 :  self.inner_dim * 3])
        elif past_key_value is None:
            key_states = shape(self.k(past_key_value[0]))
            value_states = shape(self.v(past_key_value[1]))
        if past_key_value is not None:
            if key_value_states is None:
                # self-attn
                # (batch_size, n_heads, key_length, dim_per_head)
                key_states = torch.cat([past_key_value[0], key_states], dim=2)
                value_states = torch.cat([past_key_value[1], value_states], dim=2)
            else:
                # cross-attn
                key_states = past_key_value[0]
                value_states = past_key_value[1]

        # compute scores
        scores = torch.matmul(
            query_states, key_states.transpose(3, 2)
        )  # equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9

        if position_bias is None:
            if not self.has_relative_attention_bias:
                position_bias = torch.zeros(
                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype
                )
                if self.gradient_checkpointing and self.training:
                    position_bias.requires_grad = True
            else:
                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)

            # if key and values are already calculated
            # we want only the last query position bias
            if past_key_value is not None:
                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]

            if mask is not None:
                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)

        attn_weights = torch.ops.torch_ipex.add_softmax_(scores.contiguous(), position_bias.contiguous())
        attn_weights = nn.functional.dropout(
            attn_weights, p=self.dropout, training=self.training
        )  # (batch_size, n_heads, seq_length, key_length)

        # Mask heads if we want to
        if layer_head_mask is not None:
            attn_weights = attn_weights * layer_head_mask

        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)
        attn_output = self.o(attn_output)

        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None
        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)

        if output_attentions:
            outputs = outputs + (attn_weights,)
        return outputs

def convert_model(model):
    for idx in range(len(model.encoder.block)):
        config = T5Config()
        attention = model.encoder.block[idx].layer[0].SelfAttention
        config.is_decoder = attention.is_decoder
        config.relative_attention_num_buckets = attention.relative_attention_num_buckets
        config.relative_attention_max_distance = attention.relative_attention_max_distance
        config.d_model = attention.d_model
        config.d_kv = attention.key_value_proj_dim
        config.num_heads = attention.n_heads
        config.dropout_rate = attention.dropout
        attention_opti = T5Attention(config, attention.has_relative_attention_bias)

        attention_opti.q = attention.q
        attention_opti.k = attention.k
        attention_opti.v =  attention.v
        attention_opti.o =  attention.o
        attention_opti.qkv.weight.data = torch.cat([attention_opti.q.weight, attention_opti.k.weight, attention_opti.v.weight])
        if attention_opti.q.bias is not None:
            attention_opti.qkv.bias.data = torch.cat([attention_opti.q.bias, attention_opti.k.bias, attention_opti.v.bias])
        if attention.has_relative_attention_bias:
            attention_opti.relative_attention_bias = attention.relative_attention_bias
        attention_opti.pruned_heads = attention.pruned_heads
        attention_opti.gradient_checkpointing = attention.gradient_checkpointing
        model.encoder.block[idx].layer[0].SelfAttention = attention_opti
    # decoder
    for idx in range(len(model.decoder.block)):
        config = T5Config()
        attention = model.decoder.block[idx].layer[0].SelfAttention
        config.is_decoder = attention.is_decoder
        config.relative_attention_num_buckets = attention.relative_attention_num_buckets
        config.relative_attention_max_distance = attention.relative_attention_max_distance
        config.d_model = attention.d_model
        config.d_kv = attention.key_value_proj_dim
        config.num_heads = attention.n_heads
        config.dropout_rate = attention.dropout
        attention_opti = T5Attention(config, attention.has_relative_attention_bias)

        attention_opti.q = attention.q
        attention_opti.k = attention.k
        attention_opti.v =  attention.v
        attention_opti.o =  attention.o
        attention_opti.qkv.weight.data = torch.cat([attention_opti.q.weight, attention_opti.k.weight, attention_opti.v.weight])
        if attention_opti.q.bias is not None:
            attention_opti.qkv.bias.data = torch.cat([attention_opti.q.bias, attention_opti.k.bias, attention_opti.v.bias])
        if attention.has_relative_attention_bias:
            attention_opti.relative_attention_bias = attention.relative_attention_bias
        attention_opti.pruned_heads = attention.pruned_heads
        attention_opti.gradient_checkpointing = attention.gradient_checkpointing
        model.decoder.block[idx].layer[0].SelfAttention = attention_opti
    return model

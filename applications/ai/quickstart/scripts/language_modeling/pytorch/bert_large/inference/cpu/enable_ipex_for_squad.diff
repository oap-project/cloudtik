diff --git a/examples/legacy/question-answering/run_squad.py b/examples/legacy/question-answering/run_squad.py
index fbf2ebd63..eaf700db4 100644
--- a/examples/legacy/question-answering/run_squad.py
+++ b/examples/legacy/question-answering/run_squad.py
@@ -22,6 +22,9 @@ import logging
 import os
 import random
 import timeit
+import time
+import sys
+import threading
 
 import numpy as np
 import torch
@@ -48,7 +51,11 @@ from transformers.data.metrics.squad_metrics import (
 from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor
 from transformers.trainer_utils import is_main_process
 
-
+import intel_extension_for_pytorch as ipex
+def trace_handler(prof):
+    print(prof.key_averages().table(
+        sort_by="self_cpu_time_total", row_limit=-1))
+    prof.export_chrome_trace("./log/test_trace_" + str(prof.step_num) + ".json")
 try:
     from torch.utils.tensorboard import SummaryWriter
 except ImportError:
@@ -266,6 +273,131 @@ def train(args, train_dataset, model, tokenizer):
 
     return global_step, tr_loss / global_step
 
+def wrap_model(model, args):
+    model.eval()
+    ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+    dumpy_tensor = torch.ones((args.eval_batch_size, 384), dtype=torch.long) \
+                    if not args.use_multi_stream_module \
+                    else torch.ones((args.eval_batch_size//args.num_streams, 384), dtype=torch.long)
+    jit_inputs = (dumpy_tensor, dumpy_tensor, dumpy_tensor)
+    print(args)
+    if args.int8:
+        from intel_extension_for_pytorch.quantization import prepare, convert
+        from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+        qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8), weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+        prepared_model = prepare(model, qconfig, example_inputs=jit_inputs, inplace=False)
+        prepared_model.load_qconf_summary(qconf_summary = args.int8_config)
+
+        # convert model to trace model.
+        if args.int8_fp32:
+            model = convert(prepared_model)
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+        else: #int8_bf16 mix
+            with torch.cpu.amp.autocast():
+                model = convert(prepared_model)
+                model = torch.jit.trace(model, jit_inputs, strict=False)
+        model = torch.jit.freeze(model)
+        input = {
+                 "input_ids": dumpy_tensor,
+                 "attention_mask": dumpy_tensor,
+                 "token_type_ids": dumpy_tensor,
+         }
+        # enable fusion path work(need to run two interation).
+        with torch.no_grad():
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor) 
+            #dumpy_tensor = torch.ones((128, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #dumpy_tensor = torch.ones((81, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #print("############################################")
+    elif args.bf16:
+        model = ipex.optimize(model, dtype=torch.bfloat16)
+        with torch.cpu.amp.autocast(),torch.no_grad():
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            model = torch.jit.freeze(model)
+            #model = torch.jit._recursive.wrap_cpp_module(torch._C._freeze_module(model._c, preserveParameters=True))
+            print(model.graph)
+    elif args.fp16_cpu:
+        model = ipex.optimize(model, dtype=torch.half, conv_bn_folding=False, auto_kernel_selection=True, weights_prepack=True)
+        with torch.cpu.amp.autocast(enabled=True, dtype=torch.half):
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            model = torch.jit.freeze(model)
+    elif args.bf32:
+        ipex.set_fp32_math_mode(mode=ipex.FP32MathMode.BF32, device="cpu")
+        model = ipex.optimize(model, dtype=torch.float32, level="O1", auto_kernel_selection=True)
+        with torch.no_grad():
+             model = torch.jit.trace(model, jit_inputs, strict=False)
+        model = torch.jit.freeze(model)
+    elif args.use_jit:
+        with torch.no_grad():
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            #model = torch.jit._recursive.wrap_cpp_module(torch._C._freeze_module(model._c, preserveParameters=True)) 
+            model = torch.jit.freeze(model)
+    if args.use_multi_stream_module:
+        print("Use multi stream module on numa node:{0}, num of streams:{1}, batch per stream:{2}".format(args.instance_number, args.num_streams, args.eval_batch_size//args.num_streams))
+        input_hint_object = {"input_ids": 0, "attention_mask": 0, "token_type_ids": 0}
+        multi_stream_input_hint = ipex.cpu.runtime.MultiStreamModuleHint(**input_hint_object)
+        multi_stream_output_hint = ipex.cpu.runtime.MultiStreamModuleHint((0, 0))
+        cpu_pool = ipex.cpu.runtime.CPUPool(node_id=args.instance_number)
+        model = ipex.cpu.runtime.MultiStreamModule(model,
+                                                num_streams=args.num_streams,
+                                                cpu_pool=cpu_pool,
+                                                input_split_hint = multi_stream_input_hint,
+                                                output_concat_hint = multi_stream_output_hint)
+    return model 
+
+def benchmark_evaluate(args, model, eval_dataloader):
+    steps_per_epoch = len(eval_dataloader)
+    total_steps = (args.perf_run_iters + args.perf_begin_iter)
+    test_epoches = int(total_steps / steps_per_epoch)
+    print('Evaluating BERT: Steps per Epoch {} total Steps {}'.format(steps_per_epoch, total_steps))
+    total_time = 0
+    i = 0;
+    #with torch.profiler.profile(
+    #        activities=[
+    #            torch.profiler.ProfilerActivity.CPU],
+
+    #        schedule=torch.profiler.schedule(
+    #            wait=1,
+    #            warmup=9,
+    #            active=5),
+    #        #on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/bert_bf16'),#trace_handler
+    #        on_trace_ready=trace_handler#torch.profiler.tensorboard_trace_handler('./log/bert_bf16')
+    #        # used when outputting for tensorboard
+    #        ) as prof:
+
+    with tqdm(total=total_steps, desc="Evaluating") as pbar:
+        for epoch in range(test_epoches + 1):
+            for it, batch in enumerate(eval_dataloader):
+                if epoch * steps_per_epoch + it >= total_steps:
+                    throughput = args.eval_batch_size * args.perf_run_iters / total_time
+                    print("Throughput: {:.3f} sentence/s".format(throughput))     
+                    break
+                with torch.no_grad():
+                    inputs = {
+                        "input_ids": batch[0],
+                        "attention_mask": batch[1],
+                        "token_type_ids": batch[2],
+                    }
+                    #print("---------------**inputs is:{}".format(**inputs))
+                    time_start = time.time()
+                    #print("inputs type is: {}".format(type(inputs)))
+                    #print("inputs is: {}".format(inputs))
+
+                    outputs = model(**inputs)          
+
+                    # print("outputs type is: {}".format(type(outputs)))
+                    # print("outputs len is: {}".format(len(outputs)))
+                    # print("output[0] size is: {}".format(outputs[0].size()))
+                    # print("output[1] size is: {}".format(outputs[1].size()))
+                    # print("outputs is: {}".format(outputs))
+                    #prof.step()
+                    time_end = time.time()
+                    if epoch * steps_per_epoch + it > args.perf_begin_iter:
+                        total_time +=(time_end - time_start)
+                    pbar.update(1)
+
 
 def evaluate(args, model, tokenizer, prefix=""):
     dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)
@@ -275,6 +407,7 @@ def evaluate(args, model, tokenizer, prefix=""):
 
     args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
 
+    model = wrap_model(model, args)
     # Note that DistributedSampler samples randomly
     eval_sampler = SequentialSampler(dataset)
     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
@@ -287,6 +420,40 @@ def evaluate(args, model, tokenizer, prefix=""):
     logger.info("***** Running evaluation {} *****".format(prefix))
     logger.info("  Num examples = %d", len(dataset))
     logger.info("  Batch size = %d", args.eval_batch_size)
+    if args.do_calibration:
+        import intel_extension_for_pytorch as ipex
+        from intel_extension_for_pytorch.quantization import prepare
+        from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+        qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8), weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+        dumpy_tensor = torch.ones((args.eval_batch_size, 384), dtype=torch.long)
+        jit_inputs=(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+        ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+        prepared_model = prepare(model, qconfig, example_inputs=jit_inputs, inplace=False)
+        for step, batch in enumerate(eval_dataloader):
+            print("calibration step: {}".format(step))
+            batch = {
+                "input_ids": batch[0],
+                "attention_mask": batch[1],
+                "token_type_ids": batch[2],
+            }
+            prepared_model(**batch)
+            if step == args.calibration_iters -1:
+                print("calibration finished, quantization info saved on file {}".format(args.int8_config))
+                prepared_model.save_qconf_summary(qconf_summary = args.int8_config)
+                exit()
+    if args.benchmark:
+        if args.use_share_weight:
+            threads = []
+            num_instances = args.total_cores // args.cores_per_instance
+            for i in range(0, num_instances):
+               t = threading.Thread(target=benchmark_evaluate, args=(args, model, eval_dataloader))
+               threads.append(t)
+               t.start()
+            for t in threads:
+                t.join()
+        else:
+            benchmark_evaluate(args, model, eval_dataloader)        
+        exit()
 
     all_results = []
     start_time = timeit.default_timer()
@@ -321,7 +488,7 @@ def evaluate(args, model, tokenizer, prefix=""):
             eval_feature = features[feature_index.item()]
             unique_id = int(eval_feature.unique_id)
 
-            output = [to_list(output[i]) for output in outputs.to_tuple()]
+            output = [to_list(output[i]) for output in outputs]
 
             # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other "simpler"
             # models only use two.
@@ -659,6 +826,47 @@ def main():
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")
 
     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument(
+        "--fp16_cpu",
+        action="store_true",
+        help="Whether to use fp16 16-bit (mixed) precision  instead of 32-bit on cpu")
+    parser.add_argument(
+        "--bf16",
+        action="store_true",
+        help="Whether to use 16-bit (mixed) precision  instead of 32-bit")
+    parser.add_argument("--perf_begin_iter", type=int, default=15,
+                        help="Number iterations to warm up")
+    parser.add_argument("--perf_run_iters", type=int, default=100,
+                        help="Number iterations to collection performance data begin from perf_begin_iter")
+    parser.add_argument("--iter_num", type=int, default=40,
+                        help="Number iterations to collect time")
+    parser.add_argument("--benchmark", action='store_true',
+                        help="Bench the model speed")
+    parser.add_argument("--bf32", action='store_true', help="For enabling IPEX bf32")
+    parser.add_argument("--use_jit", action='store_true', help="For jit trace")
+    parser.add_argument('--int8', dest='int8', action='store_true',
+                        help='use llga int8 in pytorch jit model')
+    parser.add_argument('--int8_fp32', dest='int8_fp32', action='store_true',
+                        help='use int8 fp32 mix precision')
+    parser.add_argument("--int8_config", type=str, default="config.json", 
+                        help="quantization config file for int8 mode")
+    parser.add_argument("--do_calibration", action='store_true',
+                        help="Enable calibration process")
+    parser.add_argument("--calibration_iters", type=int, default=100,
+                        help="Number iterations to do calibration")
+    parser.add_argument("--use_share_weight", action='store_true',
+                        help="Enable share weight mode")
+    parser.add_argument("--cores_per_instance", type=int, default=4,
+                        help="Number iterations to collect time")
+    parser.add_argument("--total_cores", type=int, default=28,
+                        help="Total cores used for this process, used for share_weight mode")
+    parser.add_argument('--use_multi_stream_module', dest='use_multi_stream_module', action='store_true',
+                        help='Whether use multi stream module for throughput mode')
+    parser.add_argument("--num_streams", type=int, default=1,
+                        help="The number of stream to use, used for multi stream module")
+    parser.add_argument("--instance_number", type=int, default=0,
+                        help="The socket to run multi stream module, used for multi stream module with multi sockets")
+
     args = parser.parse_args()
 
     if args.doc_stride >= args.max_seq_length - args.max_query_length:
@@ -730,11 +938,13 @@ def main():
     args.model_type = args.model_type.lower()
     config = AutoConfig.from_pretrained(
         args.config_name if args.config_name else args.model_name_or_path,
+        return_dict=False,
         cache_dir=args.cache_dir if args.cache_dir else None,
     )
     tokenizer = AutoTokenizer.from_pretrained(
         args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
         do_lower_case=args.do_lower_case,
+        return_dict=False,
         cache_dir=args.cache_dir if args.cache_dir else None,
         use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling
     )
diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index 6e1a41a6b..ae6cdbba2 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -323,6 +323,8 @@ class BertSelfAttention(nn.Module):
 
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
         if attention_mask is not None:
+            if attention_mask.dtype != attention_scores.dtype:
+                attention_mask = attention_mask.to(attention_scores.dtype)
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask
 
@@ -1863,9 +1865,9 @@ class BertForQuestionAnswering(BertPreTrainedModel):
             end_loss = loss_fct(end_logits, end_positions)
             total_loss = (start_loss + end_loss) / 2
 
-        if not return_dict:
-            output = (start_logits, end_logits) + outputs[2:]
-            return ((total_loss,) + output) if total_loss is not None else output
+        #if not return_dict:
+        output = (start_logits, end_logits) + outputs[2:]
+        return ((total_loss,) + output) if total_loss is not None else output
 
         return QuestionAnsweringModelOutput(
             loss=total_loss,

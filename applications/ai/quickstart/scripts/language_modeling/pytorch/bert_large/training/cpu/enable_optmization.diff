diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index 6e1a41a6b..9d310844f 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -323,6 +323,8 @@ class BertSelfAttention(nn.Module):
 
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
         if attention_mask is not None:
+            if attention_mask.dtype != attention_scores.dtype:
+                attention_mask = attention_mask.to(attention_scores.dtype)
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask
 
@@ -1037,6 +1039,7 @@ class BertForPreTraining(BertPreTrainedModel):
 
         # Initialize weights and apply final processing
         self.post_init()
+        self.dense_seq_output = config.dense_seq_output
 
     def get_output_embeddings(self):
         return self.cls.predictions.decoder
@@ -1107,12 +1110,24 @@ class BertForPreTraining(BertPreTrainedModel):
         )
 
         sequence_output, pooled_output = outputs[:2]
+        if labels is not None and self.dense_seq_output:
+            batch_size = sequence_output.shape[0]
+            seq_len = sequence_output.shape[1]
+            hidden_dim = sequence_output.shape[2]
+            sequence_flattened = torch.index_select(sequence_output.view(-1,sequence_output.shape[-1]), 0, torch.nonzero(labels.view(-1) != -100, as_tuple=False).squeeze())
+            sequence_output = sequence_flattened
+
         prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)
 
         total_loss = None
         if labels is not None and next_sentence_label is not None:
             loss_fct = CrossEntropyLoss()
-            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
+            if self.dense_seq_output:
+                labels_flat = labels.view(-1)
+                labels_dense = labels_flat[labels_flat != -100]
+                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels_dense)
+            else:
+                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
             total_loss = masked_lm_loss + next_sentence_loss
 

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba84ceb-f76f-4deb-bdf6-dce8ce016dc5",
   "metadata": {},
   "source": [
    "# Distributed Deep Learning on Spark with Hyper Parameter Tuning\n",
    "\n",
    "Spark + MLflow + Hyperopt + Horovod + Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f075d82-6228-4d7d-9119-5eb568c908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import getopt\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from distutils.version import LooseVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d54bc-dee0-45e4-8f0f-68bcc32a6d21",
   "metadata": {},
   "source": [
    "## Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4e57b-0acf-4753-8ed0-975abc91e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_batch_size = input('Please set the input batch size for training: ').strip()\n",
    "param_epochs = input('Please set the number of epochs to train: ').strip()\n",
    "param_fsdir = input('Please set the storage file system dir: ').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51f82e-21e6-4c46-9aec-cd87993ffe31",
   "metadata": {},
   "source": [
    "## CloudTik: get head ip and wait for workers ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6fdb9-7e96-48aa-a01e-9de933de34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudtik.core.api import ThisCluster\n",
    "\n",
    "cluster = ThisCluster()\n",
    "cluster_head_ip = cluster.get_head_node_ip()\n",
    "# Wait for all cluster works read\n",
    "cluster.wait_for_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a83db-b11b-4d91-8f4a-6c84736f909b",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692d6bf-03dd-4bd4-9b8c-dda983d79926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_conf = SparkConf().setAppName('spark-horovod-keras').set('spark.sql.shuffle.partitions', '16')\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "conf = spark.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90219e-64a5-4512-9a70-aaa5c84315f0",
   "metadata": {},
   "source": [
    "## Download MNIST dataset and upload to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ba47c-8f11-41a3-8fff-a22b30eb5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "data_url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2'\n",
    "mnist_data_path = os.path.join('/tmp', 'mnist.bz2')\n",
    "if not os.path.exists(mnist_data_path):\n",
    "    subprocess.check_output(['wget', data_url, '-O', mnist_data_path])\n",
    "\n",
    "# Upload to the default distributed storage\n",
    "!hadoop fs -mkdir /tmp\n",
    "!hadoop fs -put   /tmp/mnist.bz2  /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68b59-1cf5-443e-b6cd-56c51e1819d6",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd903f-4262-411c-9be4-e42acd099b28",
   "metadata": {},
   "source": [
    "### Load to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b2305-c284-498b-ab77-7e196edfb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('libsvm').option('numFeatures', '784').load(mnist_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4b651-dbd4-4892-b6fb-454e039650a9",
   "metadata": {},
   "source": [
    "### One-hot encode labels into SparseVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32678-1e27-4a9f-9ba3-7468cfc5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['label'],\n",
    "                        outputCols=['label_vec'],\n",
    "                        dropLast=False)\n",
    "model = encoder.fit(df)\n",
    "train_df = model.transform(df)\n",
    "\n",
    "# Split to train/test\n",
    "train_df, test_df = train_df.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363e38-4722-4e13-826e-63b708cef911",
   "metadata": {},
   "source": [
    "## Define training function and Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a9c35-54bf-4566-a747-1fc7059f2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import horovod.spark.keras as hvd\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from horovod.spark.common.store import Store\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84357a27-218b-4357-a126-dd498d9bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPUs when building the model to prevent memory leaks\n",
    "if LooseVersion(tf.__version__) >= LooseVersion('2.0.0'):\n",
    "    # See https://github.com/tensorflow/tensorflow/issues/33168\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    keras.backend.set_session(tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db04cab-8178-4d1e-9463-c379c9183d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "executor_cores = conf.get(\"spark.executor.cores\")\n",
    "set_num_proc = int(executor_cores)\n",
    "print(set_num_proc)\n",
    "\n",
    "set_batch_size = int(param_batch_size) if param_batch_size else 128\n",
    "print(set_batch_size)\n",
    "\n",
    "set_epochs = int(param_epochs) if param_epochs else 1\n",
    "print(set_epochs)\n",
    "\n",
    "store_path = param_fsdir + \"/tmp\"\n",
    "store = Store.create(store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffb876-d9b5-46ed-9642-ab7b01e1d566",
   "metadata": {},
   "source": [
    "###  Horovod distributed training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d74fd7-353b-4928-b58f-1d65d812a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta(learning_rate)\n",
    "    loss = keras.losses.categorical_crossentropy\n",
    "\n",
    "    backend = SparkBackend(num_proc=set_num_proc,\n",
    "                       stdout=sys.stdout, stderr=sys.stderr,\n",
    "                       prefix_output_with_timestamp=True)\n",
    "    keras_estimator = hvd.KerasEstimator(backend=backend,\n",
    "                                         store=store,\n",
    "                                         model=model,\n",
    "                                         optimizer=optimizer,\n",
    "                                         loss=loss,\n",
    "                                         metrics=['accuracy'],\n",
    "                                         feature_cols=['features'],\n",
    "                                         label_cols=['label_vec'],\n",
    "                                         batch_size=set_batch_size,\n",
    "                                         epochs=set_epochs,\n",
    "                                         verbose=1)\n",
    "\n",
    "    keras_model = keras_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b1c4e-a9e2-47fc-b708-5450c495fe52",
   "metadata": {},
   "source": [
    "###  Hyperopt objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4119a4-04f9-458b-9254-8d3daca33935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_objective(learning_rate):\n",
    "    keras_model = train(learning_rate)\n",
    "    pred_df = keras_model.transform(test_df)\n",
    "    argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "    pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "    \n",
    "    accuracy = evaluator.evaluate(pred_df)\n",
    "    print('Test accuracy:', accuracy)\n",
    "    with mlflow.start_run():\n",
    "      mlflow.log_metric(\"learning_rate\", learning_rate)\n",
    "      mlflow.log_metric(\"loss\", 1-accuracy)\n",
    "    return {'loss': 1-accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce39a92-23a9-4df0-8ab4-9679dcc65405",
   "metadata": {},
   "source": [
    "## Do a super parameter tuning with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7d3e8-5066-4d7f-b8f5-427265053507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "\n",
    "search_space = hp.uniform('learning_rate', 0, 1)\n",
    "mlflow.set_tracking_uri(f\"http://{cluster_head_ip}:5001\")\n",
    "mlflow.set_experiment(\"MNIST: Spark + Horovod + Hyperopt\")\n",
    "argmin = fmin(\n",
    "    fn=hyper_objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=16)\n",
    "print(\"Best value found: \", argmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49170c5-518c-4eb3-b29f-d0a0354bd139",
   "metadata": {},
   "source": [
    "## Train final model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae27969-3fd9-42c2-8d71-8038aabfca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train(argmin.get('learning_rate'))\n",
    "metadata = best_model._get_metadata()\n",
    "floatx = best_model._get_floatx()\n",
    "mlflow.keras.log_model(best_model.getModel(), \"Keras-Sequential-model\",registered_model_name=\"Keras-Sequential-model-reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df1856-723c-4840-a2f6-a989a5e64cb5",
   "metadata": {},
   "source": [
    "## Load the model from MLflow and run a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021eb1-033f-47cf-92e8-be79b9b9ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"models:/Keras-Sequential-model-reg/1\"\n",
    "loaded_model = mlflow.keras.load_model(model_uri)\n",
    "\n",
    "hvd_keras_model = hvd.KerasModel(model=loaded_model,\n",
    "                                 feature_columns=['features'],\n",
    "                                 label_columns=['label_vec'],\n",
    "                                 _floatx = floatx,\n",
    "                                 _metadata = metadata)\n",
    "\n",
    "pred_df = hvd_keras_model.transform(test_df)\n",
    "pred_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738db54-f25c-443d-be80-1b78a5c5532e",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9770562-afc7-44b6-b7d0-53885c847958",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba84ceb-f76f-4deb-bdf6-dce8ce016dc5",
   "metadata": {},
   "source": [
    "# Distributed Deep Learning on Spark with Hyper Parameter Tuning\n",
    "\n",
    "Spark + MLflow + Hyperopt + Horovod + Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f075d82-6228-4d7d-9119-5eb568c908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import getopt\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from distutils.version import LooseVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d54bc-dee0-45e4-8f0f-68bcc32a6d21",
   "metadata": {},
   "source": [
    "## Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4e57b-0acf-4753-8ed0-975abc91e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input('Please set the input batch size for training: ').strip()\n",
    "epochs = input('Please set the number of epochs to train: ').strip()\n",
    "fsdir = input('Please set the storage file system dir: ').strip()\n",
    "trials = input('Please set the number of trials of parameter tuning: ').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51f82e-21e6-4c46-9aec-cd87993ffe31",
   "metadata": {},
   "source": [
    "## CloudTik: scale workers and wait for workers ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6fdb9-7e96-48aa-a01e-9de933de34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudtik.runtime.spark.api import ThisSparkCluster\n",
    "from cloudtik.runtime.ai.api import ThisAICluster\n",
    "\n",
    "cluster = ThisSparkCluster()\n",
    "\n",
    "# Scale the cluster as need\n",
    "# cluster.scale(workers=1)\n",
    "\n",
    "# Wait for all cluster workers to be ready\n",
    "cluster.wait_for_ready(min_workers=1)\n",
    "\n",
    "# Total worker cores\n",
    "cluster_info = cluster.get_info()\n",
    "\n",
    "args_num_proc = 1\n",
    "total_worker_cpus = cluster_info.get('total-worker-cpus-ready')\n",
    "if total_worker_cpus:\n",
    "    args_num_proc = int(total_worker_cpus / 2)\n",
    "    if not args_num_proc:\n",
    "        args_num_proc = 1\n",
    "\n",
    "default_storage = cluster.get_default_storage()\n",
    "if not fsdir:\n",
    "    fsdir = default_storage.get('default.storage.uri') if default_storage else None\n",
    "\n",
    "ai_cluster = ThisAICluster()\n",
    "mlflow_url = ai_cluster.get_endpoints()['mlflow']['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a83db-b11b-4d91-8f4a-6c84736f909b",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692d6bf-03dd-4bd4-9b8c-dda983d79926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('spark-horovod-keras').set('spark.sql.shuffle.partitions', '16')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90219e-64a5-4512-9a70-aaa5c84315f0",
   "metadata": {},
   "source": [
    "## Download MNIST dataset and upload to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ba47c-8f11-41a3-8fff-a22b30eb5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "data_url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2'\n",
    "mnist_data_path = os.path.join('/tmp', 'mnist.bz2')\n",
    "if not os.path.exists(mnist_data_path):\n",
    "    subprocess.check_output(['wget', data_url, '-O', mnist_data_path])\n",
    "\n",
    "# Upload to the default distributed storage\n",
    "!hadoop fs -mkdir /tmp\n",
    "!hadoop fs -put   /tmp/mnist.bz2  /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68b59-1cf5-443e-b6cd-56c51e1819d6",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd903f-4262-411c-9be4-e42acd099b28",
   "metadata": {},
   "source": [
    "### Load to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b2305-c284-498b-ab77-7e196edfb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('libsvm').option('numFeatures', '784').load(mnist_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4b651-dbd4-4892-b6fb-454e039650a9",
   "metadata": {},
   "source": [
    "### One-hot encode labels into SparseVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32678-1e27-4a9f-9ba3-7468cfc5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['label'],\n",
    "                        outputCols=['label_vec'],\n",
    "                        dropLast=False)\n",
    "model = encoder.fit(df)\n",
    "train_df = model.transform(df)\n",
    "\n",
    "# Split to train/test\n",
    "train_df, test_df = train_df.randomSplit([0.9, 0.1])\n",
    "if train_df.rdd.getNumPartitions() < args_num_proc:\n",
    "    train_df = train_df.repartition(args_num_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363e38-4722-4e13-826e-63b708cef911",
   "metadata": {},
   "source": [
    "## Define training function and Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a9c35-54bf-4566-a747-1fc7059f2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import horovod.spark.keras as hvd\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from horovod.spark.common.store import Store\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84357a27-218b-4357-a126-dd498d9bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPUs when building the model to prevent memory leaks\n",
    "if LooseVersion(tf.__version__) >= LooseVersion('2.0.0'):\n",
    "    # See https://github.com/tensorflow/tensorflow/issues/33168\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    keras.backend.set_session(tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db04cab-8178-4d1e-9463-c379c9183d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "num_proc = args_num_proc\n",
    "print('Train processes: {}'.format(num_proc))\n",
    "\n",
    "batch_size = int(batch_size) if batch_size else 128\n",
    "print('Train batch size: {}'.format(batch_size))\n",
    "\n",
    "epochs = int(epochs) if epochs else 1\n",
    "print('Train epochs: {}'.format(epochs))\n",
    "\n",
    "# Create store for data accessing\n",
    "store_path = fsdir + \"/tmp\"\n",
     "# AWS and GCP cloud storage authentication just work with empty storage options\n",
     "# Azure cloud storage authentication needs a few options\n",
     "storage_options = {}\n",
     "if default_storage and 'azure.storage.accoun' in default_storage:\n",
     "    storage_options['anon'] = False\n",
     "    storage_options['account_name'] = default_storage['azure.storage.account']\n",
    "store = Store.create(store_path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffb876-d9b5-46ed-9642-ab7b01e1d566",
   "metadata": {},
   "source": [
    "###  Horovod distributed training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d74fd7-353b-4928-b58f-1d65d812a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta(learning_rate)\n",
    "    loss = keras.losses.categorical_crossentropy\n",
    "\n",
    "    backend = SparkBackend(num_proc=num_proc,\n",
    "                       stdout=sys.stdout, stderr=sys.stderr,\n",
    "                       prefix_output_with_timestamp=True)\n",
    "    keras_estimator = hvd.KerasEstimator(backend=backend,\n",
    "                                         store=store,\n",
    "                                         model=model,\n",
    "                                         optimizer=optimizer,\n",
    "                                         loss=loss,\n",
    "                                         metrics=['accuracy'],\n",
    "                                         feature_cols=['features'],\n",
    "                                         label_cols=['label_vec'],\n",
    "                                         batch_size=batch_size,\n",
    "                                         epochs=epochs,\n",
    "                                         verbose=1)\n",
    "\n",
    "    keras_model = keras_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b1c4e-a9e2-47fc-b708-5450c495fe52",
   "metadata": {},
   "source": [
    "###  Hyperopt objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4119a4-04f9-458b-9254-8d3daca33935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def hyper_objective(learning_rate):\n",
    "    with mlflow.start_run():\n",
    "        keras_model = train(learning_rate)\n",
    "        pred_df = keras_model.transform(test_df)\n",
    "        argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "        pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "        accuracy = evaluator.evaluate(pred_df)\n",
    "        print('Test accuracy:', accuracy)\n",
    "        \n",
    "        mlflow.log_metric(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_metric(\"loss\", 1-accuracy)\n",
    "    return {'loss': 1-accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce39a92-23a9-4df0-8ab4-9679dcc65405",
   "metadata": {},
   "source": [
    "## Do a super parameter tuning with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7d3e8-5066-4d7f-b8f5-427265053507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "trials = int(trials) if trials else 2\n",
    "print('Hyper parameter tuning trials: {}'.format(trials))\n",
    "\n",
    "search_space = hp.uniform('learning_rate', 0, 1)\n",
    "mlflow.set_tracking_uri(mlflow_url)\n",
    "mlflow.set_experiment(\"MNIST: Spark + Horovod + Hyperopt\")\n",
    "argmin = fmin(\n",
    "    fn=hyper_objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=trials)\n",
    "print(\"Best parameter found: \", argmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49170c5-518c-4eb3-b29f-d0a0354bd139",
   "metadata": {},
   "source": [
    "## Train final model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae27969-3fd9-42c2-8d71-8038aabfca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train(argmin.get('learning_rate'))\n",
    "metadata = best_model._get_metadata()\n",
    "floatx = best_model._get_floatx()\n",
    "model_name = 'keras-mnist-model'\n",
    "mlflow.keras.log_model(best_model.getModel(), model_name, registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df1856-723c-4840-a2f6-a989a5e64cb5",
   "metadata": {},
   "source": [
    "## Load the model from MLflow and run a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021eb1-033f-47cf-92e8-be79b9b9ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = 'models:/{}/latest'.format(model_name)\n",
    "print('Inference with model: {}'.format(model_uri))\n",
    "saved_keras_model = mlflow.keras.load_model(model_uri)\n",
    "\n",
    "saved_model = hvd.KerasModel(model=saved_keras_model,\n",
    "                                 feature_columns=['features'],\n",
    "                                 label_columns=['label_vec'],\n",
    "                                 _floatx=floatx,\n",
    "                                 _metadata=metadata).setOutputCols(['label_prob'])\n",
    "\n",
    "pred_df = saved_model.transform(test_df)\n",
    "argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(pred_df)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "pred_df = pred_df.sampleBy('label', fractions={0.0: 0.1, 1.0: 0.1, 2.0: 0.1, 3.0: 0.1, 4.0: 0.1, 5.0: 0.1, 6.0: 0.1, 7.0: 0.1, 8.0: 0.1, 9.0: 0.1})\n",
    "pred_df.show(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738db54-f25c-443d-be80-1b78a5c5532e",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9770562-afc7-44b6-b7d0-53885c847958",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Machine Learning on Spark with Hyper Parameter Tuning\n",
    "\n",
    "Spark + MLflow + Hyperopt + scikit-learn\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for hyperparameter tuning, including automated MLflow tracking and the `SparkTrials` class for distributed tuning.  \n",
    "\n",
    "This notebook illustrates how to scale up hyperparameter tuning for a single-machine Python ML algorithm and track the results using MLflow. You learn to use the `SparkTrials` class to distribute the workflow calculations across the Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d54bc-dee0-45e4-8f0f-68bcc32a6d21",
   "metadata": {},
   "source": [
    "## Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4e57b-0acf-4753-8ed0-975abc91e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = input('Please set the number of trials of parameter tuning: ').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudTik: scale workers and wait for workers ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudtik.runtime.spark.api import ThisSparkCluster\n",
    "from cloudtik.runtime.ml.api import ThisMLCluster\n",
    "\n",
    "cluster = ThisSparkCluster()\n",
    "\n",
    "# Scale the cluster as need\n",
    "# cluster.scale(workers=1)\n",
    "\n",
    "# Wait for all cluster workers to be ready\n",
    "cluster.wait_for_ready(min_workers=1)\n",
    "\n",
    "ml_cluster = ThisMLCluster()\n",
    "mlflow_url = ml_cluster.get_services()['mlflow']['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('spark-scikit').set('spark.sql.shuffle.partitions', '16')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the iris dataset from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Function to train a model\n",
    "def train(C):\n",
    "    # Create a support vector classifier model\n",
    "    model = SVC(C=C)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_objective(C):\n",
    "    # Create a support vector classifier model\n",
    "    model = train(C)\n",
    "\n",
    "    # Use the cross-validation accuracy to compare the models' performance\n",
    "    accuracy = cross_val_score(model, X, y).mean()\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metric(\"C\", C)\n",
    "        mlflow.log_metric(\"loss\", -accuracy)\n",
    "\n",
    "    # Hyperopt tries to minimize the objective function.\n",
    "    # A higher accuracy value means a better model, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'C': C, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a super parameter tuning with hyperopt\n",
    "\n",
    "Here are the steps in a Hyperopt workflow:  \n",
    "1. Define a function to minimize.  \n",
    "2. Define a search space over hyperparameters.  \n",
    "3. Select a search algorithm.  \n",
    "4. Run the tuning algorithm with Hyperopt `fmin()`.\n",
    "\n",
    "Define the search space over hyperparameters:\n",
    "See the [Hyperopt docs](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions) for details on defining a search space and parameter expressions.\n",
    "\n",
    "Search algorithm, the two main choices are:\n",
    "* `hyperopt.tpe.suggest`: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on past results\n",
    "* `hyperopt.rand.suggest`: Random search, a non-adaptive approach that samples over the search space\n",
    "\n",
    "Run the tuning algorithm with Hyperopt `fmin()`\n",
    "\n",
    "Set `max_evals` to the maximum number of points in hyperparameter space to test, that is, the maximum number of models to fit and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "import mlflow\n",
    "\n",
    "trials = int(trials) if trials else 2\n",
    "print('Hyper parameter tuning trials: {}'.format(trials))\n",
    "\n",
    "# Define the search space and select a search algorithm\n",
    "search_space = hp.lognormal('C', 0, 1.0)\n",
    "algo = tpe.suggest\n",
    "spark_trials = SparkTrials(spark_session=spark)\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_url)\n",
    "mlflow.set_experiment(\"MLflow + HyperOpt + Scikit-Learn\")\n",
    "argmin = fmin(\n",
    "  fn=hyper_objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=trials,\n",
    "  trials=spark_trials)\n",
    "\n",
    "# Print the best value found for C\n",
    "print(\"Best parameter found: \", argmin)\n",
    "print(\"argmin.get('C'): \", argmin.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train(argmin.get('C'))\n",
    "model_name = 'scikit-learn-svc-model'\n",
    "mlflow.sklearn.log_model(best_model, model_name, registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model as a PyFuncModel and predict on a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_uri = 'models:/{}/latest'.format(model_name)\n",
    "print('Inference with model: {}'.format(model_uri))\n",
    "saved_model = mlflow.pyfunc.load_model(model_uri)\n",
    "saved_model.predict(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

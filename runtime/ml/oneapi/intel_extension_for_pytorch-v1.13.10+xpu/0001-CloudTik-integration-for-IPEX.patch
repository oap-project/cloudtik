From 4958281ec8959bf7edb5eae7551db2192d737068 Mon Sep 17 00:00:00 2001
From: haojinIntel <hao.jin@intel.com>
Date: Wed, 29 Mar 2023 11:52:42 +0800
Subject: [PATCH] Enhance numactl for different platforms

---
 intel_extension_for_pytorch/cpu/launch.py | 120 ++++++++++++++++++----
 1 file changed, 98 insertions(+), 22 deletions(-)

diff --git a/intel_extension_for_pytorch/cpu/launch.py b/intel_extension_for_pytorch/cpu/launch.py
index 19fcb89c..8fd7aab5 100644
--- a/intel_extension_for_pytorch/cpu/launch.py
+++ b/intel_extension_for_pytorch/cpu/launch.py
@@ -123,14 +123,20 @@ rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*
 class CPUinfo():
     '''
     Get CPU inforamation, such as cores list and NUMA information.
+    If host_ip is not None, we will use `cloudtik head exec --node-ip ip` command to get lscpu info.
     '''
-    def __init__(self):
+    def __init__(self, host_ip=None):
 
         self.cpuinfo = []
         if platform.system() == "Windows":
             raise RuntimeError("Windows platform is not supported!!!")
         elif platform.system() == "Linux":
-            args = ["lscpu", "--parse=CPU,Core,Socket,Node"]
+            # CloudTik: patch start
+            if host_ip is None:
+                args = ["lscpu", "--parse=CPU,Core,Socket,Node"]
+            else:
+                args =["cloudtik", "head", "exec", "--node-ip", host_ip, "lscpu --parse=CPU,Core,Socket,Node"]
+            # CloudTik: patch end
             env_lang = os.getenv('LANG', 'UNSET')
             os.environ['LANG'] = 'C'
             lscpu_info = subprocess.check_output(args, env=os.environ, universal_newlines=True).split("\n")
@@ -161,15 +167,24 @@ class CPUinfo():
         for node_id in range(self.nodes):
             cur_node_physical_core = []
             cur_node_logical_core = []
+            # CloudTik: patch start
+            cur_node_physical_core_cpu_id = []
+            # CloudTik: patch end
             for line in self.cpuinfo:
                 nid = line[idx_active] if line[idx_active] != '' else '0'
                 if node_id == int(nid):
                     if int(line[1]) not in cur_node_physical_core:
                         cur_node_physical_core.append(int(line[1]))
+                        # CloudTik: patch start
+                        cur_node_physical_core_cpu_id.append(int(line[0]))
+                        # CloudTik: patch end
                         self.physical_core_node_map[int(line[1])] = int(node_id)
                     cur_node_logical_core.append(int(line[0]))
                     self.logical_core_node_map[int(line[0])] = int(node_id)
-            self.node_physical_cores.append(cur_node_physical_core)
+            # CloudTik: patch start
+            self.node_physical_cores.append(cur_node_physical_core_cpu_id)
+            # self.node_physical_cores.append(cur_node_physical_core)
+            # CloudTik: patch end
             self.node_logical_cores.append(cur_node_logical_core)
 
     def node_nums(self):
@@ -551,7 +566,7 @@ class DistributedTrainingLauncher(Launcher):
     r"""
      Launcher for distributed traning with MPI launcher
      """
-    def get_mpi_pin_domain(self, nproc_per_node, ccl_worker_count, total_cores):
+    def get_mpi_pin_domain(self, nproc_per_node, ccl_worker_count, total_cores, flatten_node_cores):
         '''
         I_MPI_PIN_DOMAIN specify the cores used for every MPI process.
         The first ccl_worker_count cores of every rank for ccl communication
@@ -563,18 +578,22 @@ class DistributedTrainingLauncher(Launcher):
         '''
         ppn = nproc_per_node
         cores_per_rank = total_cores // ppn
+
         pin_domain = "["
         for proc in range(ppn):
             domain_binary = 0
             begin = proc * cores_per_rank + ccl_worker_count
             end = proc * cores_per_rank + cores_per_rank - 1
             for i in range(begin, end + 1):
-                domain_binary |= (1 << i)
+            # CloudTik: patch start
+                domain_binary |= (1 << flatten_node_cores[i])
+                # domain_binary |= (1 << i)
+            # CloudTik: patch end
             pin_domain += hex(domain_binary) + ","
         pin_domain += "]"
         return pin_domain
 
-    def get_ccl_worker_affinity(self, nproc_per_node, ccl_worker_count, total_cores):
+    def get_ccl_worker_affinity(self, nproc_per_node, ccl_worker_count, total_cores, flatten_node_cores):
         '''
         Computation and communication use different cores when using oneCCL
         backend for distributed training. we use first ccl_worker_count cores of
@@ -585,7 +604,10 @@ class DistributedTrainingLauncher(Launcher):
         affinity = ''
         for proc in range(ppn):
             for ccl_worker in range(ccl_worker_count):
-                affinity += str(proc * cores_per_rank + ccl_worker) + ","
+                # CloudTik: patch start
+                affinity += str(flatten_node_cores[proc * cores_per_rank + ccl_worker]) + ","
+                # affinity += str(proc * cores_per_rank + ccl_worker) + ","
+                # CloudTik: patch end
         affinity = affinity[:-1]
         return affinity
 
@@ -593,10 +615,10 @@ class DistributedTrainingLauncher(Launcher):
         '''
         Set ENVs and launch MPI process for distributed training.
         '''
-        if args.nnodes > 1 and not os.path.exists(args.hostfile):
+        if not args.hosts and args.nnodes > 1 and not os.path.exists(args.hostfile):
             raise ValueError("hostfile is necessary when you use multi-node distributed training,"
                              "Please create hostfile which include the ip list you used for distributed running")
-        elif args.nnodes > 1:
+        elif not args.hosts and args.nnodes > 1:
             ipv4_addr_pattern = r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
             ip_list = []
             with open(args.hostfile) as f:
@@ -632,15 +654,47 @@ class DistributedTrainingLauncher(Launcher):
                     exit(-1)
                 else:
                     logger.info("connection from master node {} to slave node {} is OK".format(args.master_addr, ip))
-
-        total_cores_per_node = self.cpuinfo.physical_core_nums()
-        if args.use_logical_core:
-            total_cores_per_node = self.cpuinfo.logical_core_nums()
+        else:
+            # CloudTik: patch start
+            if args.hosts:
+                host_list = args.hosts.split(',')
+                args.nnodes = len(host_list)
+                args.master_addr = host_list[0]
+            # CloudTik: patch end
+
+        if not args.hosts:
+            node_cores = self.cpuinfo.node_physical_cores
+            total_cores_per_node = self.cpuinfo.physical_core_nums()
+            if args.use_logical_core:
+                node_cores = self.cpuinfo.node_logical_cores
+                total_cores_per_node = self.cpuinfo.logical_core_nums()
+            if args.nproc_per_node == 0:
+                args.nproc_per_node = self.cpuinfo.node_nums()
+        else:
+            # CloudTik: patch start
+            total_cores_per_node = args.cores_per_node
+            host_list = args.hosts.split(',')
+            remote_cpuinfo = CPUinfo(host_ip=host_list[-1])
+            node_cores = remote_cpuinfo.node_physical_cores
+            if args.use_logical_core:
+                node_cores = remote_cpuinfo.node_logical_cores
+            if not total_cores_per_node:
+                total_cores_per_node = remote_cpuinfo.physical_core_nums()
+                if args.use_logical_core:
+                    total_cores_per_node = remote_cpuinfo.logical_core_nums()
+            if args.nproc_per_node == 0:
+                args.nproc_per_node = remote_cpuinfo.node_nums()
+        flatten_node_cores = []
+        for node_numa_cores in node_cores:
+            flatten_node_cores.extend(node_numa_cores)
+            # CloudTik: patch end
 
         # set distributed related environmental variables
         self.set_env("MASTER_ADDR", args.master_addr)
         self.set_env("MASTER_PORT", str(args.master_port))
-        mpi_pin_domain = self.get_mpi_pin_domain(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node)
+        # CloudTik: patch start
+        mpi_pin_domain = self.get_mpi_pin_domain(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
+        # CloudTik: patch end
         self.set_env("I_MPI_PIN_DOMAIN", mpi_pin_domain)
 
         ppn = args.nproc_per_node
@@ -655,15 +709,31 @@ class DistributedTrainingLauncher(Launcher):
                                             args.use_default_allocator)
 
         self.set_env("CCL_WORKER_COUNT", str(args.ccl_worker_count))
-        ccl_affinity = self.get_ccl_worker_affinity(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node)
+        ccl_affinity = self.get_ccl_worker_affinity(args.nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
         self.set_env("CCL_WORKER_AFFINITY", ccl_affinity)
 
         os.environ["LAUNCH_CMD"] = "#"
         cmd = ['mpiexec.hydra']
         mpi_config = "-l -np {} -ppn {} -genv I_MPI_PIN_DOMAIN={} -genv OMP_NUM_THREADS={} ".format(args.nnodes * args.nproc_per_node, args.nproc_per_node, mpi_pin_domain, omp_num_threads)
         mpi_config += args.more_mpi_params
-        if args.nnodes > 1:
-            mpi_config += " -hostfile {}".format(args.hostfile)
+
+        # CloudTik: patch start
+        if args.hosts:
+            mpi_config += " -hosts {}".format(args.hosts)
+        else:
+            if args.nnodes > 1:
+                mpi_config += " -hostfile {}".format(args.hostfile)
+
+        def get_cloudtik_rsh():
+            cloudtik_home = os.path.join(
+                os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'cloudtik')
+            return os.path.join(cloudtik_home, "runtime/ml/scripts", "cloudtik-rsh.sh")
+
+        if "-launcher-exec" not in mpi_config:
+            mpi_config += (
+                ' -launcher rsh -launcher-exec "{launcher_exec}"'.format(launcher_exec=get_cloudtik_rsh()))
+        # CloudTik: patch end
+
         cmd.extend(mpi_config.split())
         with_python = not args.no_python
         if with_python:
@@ -686,14 +756,11 @@ class DistributedTrainingLauncher(Launcher):
 
 def add_distributed_training_params(parser):
 
-    cpuinfo = CPUinfo()
-    node_nums = cpuinfo.node_nums()
-
     group = parser.add_argument_group("Distributed Training Parameters With oneCCL backend")
     group.add_argument("--nnodes", metavar='\b', type=int, default=1,
                        help="The number of nodes to use for distributed "
                        "training")
-    group.add_argument("--nproc_per_node", metavar='\b', type=int, default=node_nums,
+    group.add_argument("--nproc_per_node", metavar='\b', type=int, default=0,
                        help="The number of processes to launch on each node")
     # ccl control
     group.add_argument("--ccl_worker_count", metavar='\b', default=4, type=int,
@@ -713,6 +780,15 @@ def add_distributed_training_params(parser):
                             "training. hostfile includes the node address list "
                             "node address which should be either the IP address"
                             "or the hostname.")
+    # CloudTik: patch start
+    group.add_argument("--hosts", metavar='\b', default="", type=str,
+                       help="List of hosts separated with comma for launching tasks. "
+                            "When hosts is specified, it implies distributed training. "
+                            "node address which should be either the IP address"
+                            "or the hostname.")
+    group.add_argument("--cores_per_node", metavar='\b', type=int, default=0,
+                       help="The number of cores for each node")
+    # CloudTik: patch end
     group.add_argument("--more_mpi_params", metavar='\b', default="", type=str,
                        help="User can pass more parameters for mpiexec.hydra "
                             "except for -np -ppn -hostfile and -genv I_MPI_PIN_DOMAIN")
@@ -849,7 +925,7 @@ def main():
     if args.latency_mode and args.throughput_mode:
         raise RuntimeError("Either args.latency_mode or args.throughput_mode should be set")
 
-    if args.nnodes > 1:
+    if args.nnodes > 1 or args.hosts:
         args.distributed = True
 
     if not args.no_python and not args.program.endswith(".py"):
-- 
2.20.1


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba84ceb-f76f-4deb-bdf6-dce8ce016dc5",
   "metadata": {},
   "source": [
    "# A Horovod with HyperOpt + MLflow integration example on Keras + Spark + Mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f075d82-6228-4d7d-9119-5eb568c908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "if LooseVersion(pyspark.__version__) < LooseVersion('3.0.0'):\n",
    "    from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder\n",
    "else:\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import horovod.spark.keras as hvd\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from horovod.spark.common.store import Store\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a83db-b11b-4d91-8f4a-6c84736f909b",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692d6bf-03dd-4bd4-9b8c-dda983d79926",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('keras_spark_mnist').set('spark.sql.shuffle.partitions', '16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e7f56-60fe-43e5-868d-c4364cfec31a",
   "metadata": {},
   "source": [
    "##### Get Spark Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc06eb-00fe-4500-b202-b106dbee81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = !echo $SPARK_HOME/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5501e90-5621-481e-b100-b5803a71d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_conf.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ca1ee-1b0b-4d0f-9732-bacce32691f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master():\n",
    "    with open(spark_conf.s) as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('#') and line.split():\n",
    "                line = line.split()\n",
    "                if line[0] == \"spark.master\":\n",
    "                        spark_master = line[1]\n",
    "                        return spark_master\n",
    "\n",
    "set_master = get_master()\n",
    "\n",
    "print(set_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d911a78-3a69-4207-afff-e08b47145f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.setMaster(set_master)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837503d2-b5b9-4054-bf5e-651005a3ac67",
   "metadata": {},
   "source": [
    "#### Setup our store for intermediate data and Download Mnist dataset and upload to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91766e-6fa3-4ba3-9b52-952f4264e344",
   "metadata": {},
   "source": [
    "##### Get Master IP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddfcb1-b564-4b17-bf48-7d939913349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_ip = !hostname -I | awk '{print $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41fdb9-ef01-4e04-aafc-161b82ed7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5342fc-dd85-497b-b5f7-80a52b174ec8",
   "metadata": {},
   "source": [
    "##### Set up our store HDFS path for intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad574678-8665-42ca-83fa-b0f7267e989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://\" + master_ip.s + \":9000/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682818e-ce3a-4a7e-81be-3bdc83884ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15ba28-40fa-4fd1-9ea8-a7a7631b014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Store.create(hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d0bc1-7d87-46a1-9ea9-88a4213dcba4",
   "metadata": {},
   "source": [
    "##### Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacba13d-c80b-4371-b413-26449e148cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2'\n",
    "libsvm_path = os.path.join('/tmp', 'mnist.bz2')\n",
    "if not os.path.exists(libsvm_path):\n",
    "    subprocess.check_output(['wget', data_url, '-O', libsvm_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ff762-ec3d-4dab-9f8e-53ae08c380b6",
   "metadata": {},
   "source": [
    "##### Upload Mnist dataset to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ede0f-83ea-425d-a325-c1959a6414a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir /tmp\n",
    "!hadoop fs -put   /tmp/mnist.bz2  /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68b59-1cf5-443e-b6cd-56c51e1819d6",
   "metadata": {},
   "source": [
    "## Load dataset into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b2305-c284-498b-ab77-7e196edfb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('libsvm') \\\n",
    "    .option('numFeatures', '784') \\\n",
    "    .load(libsvm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4b651-dbd4-4892-b6fb-454e039650a9",
   "metadata": {},
   "source": [
    "## One-hot encode labels into SparseVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c32678-1e27-4a9f-9ba3-7468cfc5419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCols=['label'],\n",
    "                        outputCols=['label_vec'],\n",
    "                        dropLast=False)\n",
    "model = encoder.fit(df)\n",
    "train_df = model.transform(df)\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = train_df.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84357a27-218b-4357-a126-dd498d9bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPUs when building the model to prevent memory leaks\n",
    "if LooseVersion(tf.__version__) >= LooseVersion('2.0.0'):\n",
    "    # See https://github.com/tensorflow/tensorflow/issues/33168\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    keras.backend.set_session(tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50110b3d-66ee-41a4-b00c-4898443ae94a",
   "metadata": {},
   "source": [
    "##   Set Traing parameters\n",
    "##### Set the number of worker processes for training\n",
    "\n",
    "Please set number of worker processes for training, default: `spark.executor.cores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ba7c0-2ad9-4393-a6ed-215c9159f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_executor_cores():\n",
    "    with open(spark_conf.s) as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('#') and line.split():\n",
    "                line = line.split()\n",
    "                if line[0] == \"spark.executor.cores\":\n",
    "                        spark_executor_cores = line[1]\n",
    "                        return spark_executor_cores\n",
    "\n",
    "executor_cores = get_executor_cores()\n",
    "set_num_proc = int(executor_cores)\n",
    "\n",
    "# input_proc = input('Please set number of worker processes for training: ').strip()\n",
    "# set_num_proc = int(input_proc) if input_proc else executor_cores\n",
    "print(set_num_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a25fec-8d3e-4b66-9674-356505dccd0f",
   "metadata": {},
   "source": [
    "##### Set the batch size\n",
    "input batch size for training, default: `128`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d4494-56bc-4955-88b6-88d8801c1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input('Please set the input batch size for training: ').strip()\n",
    "set_batch_size = int(input_size) if input_size else 128\n",
    "\n",
    "print(set_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b641abc-8ed4-49ad-b3d5-c2499344ae3c",
   "metadata": {},
   "source": [
    "##### Set the number of epochs to train\n",
    "number of epochs to train, default: `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18540276-fa1f-43c3-94c2-3b07aeaffd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_epochs = input('Please set the number of epochs to train: ').strip()\n",
    "set_epochs = int(input_epochs) if input_epochs else 1\n",
    "\n",
    "print(set_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0a826-b58a-4c9e-996f-894825c8b0c7",
   "metadata": {},
   "source": [
    "## Define the Keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e25cd-0e52-4230-9b72-a2a0d908785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta(learning_rate)\n",
    "    loss = keras.losses.categorical_crossentropy\n",
    "    \n",
    "    \n",
    "    backend = SparkBackend(num_proc=set_num_proc,\n",
    "                       stdout=sys.stdout, stderr=sys.stderr,\n",
    "                       prefix_output_with_timestamp=True)\n",
    "    keras_estimator = hvd.KerasEstimator(backend=backend,\n",
    "                                         store=store,\n",
    "                                         model=model,\n",
    "                                         optimizer=optimizer,\n",
    "                                         loss=loss,\n",
    "                                         metrics=['accuracy'],\n",
    "                                         feature_cols=['features'],\n",
    "                                         label_cols=['label_vec'],\n",
    "                                         batch_size=set_batch_size,\n",
    "                                         epochs=set_epochs,\n",
    "                                         verbose=1)\n",
    "\n",
    "    keras_model = keras_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "    \n",
    "    pred_df = keras_model.transform(test_df)\n",
    "    argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "    pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "    \n",
    "    accuracy = evaluator.evaluate(pred_df)\n",
    "    print('Test accuracy:', accuracy)\n",
    "    with mlflow.start_run():\n",
    "      mlflow.log_metric(\"learning_rate\", learning_rate)\n",
    "      mlflow.log_metric(\"loss\", 1-accuracy)\n",
    "    return {'loss': 1-accuracy, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae27969-3fd9-42c2-8d71-8038aabfca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "search_space = hp.uniform('learning_rate', 0, 1)\n",
    "mlflow.set_tracking_uri(f\"http://{master_ip}:5001\")\n",
    "mlflow.set_experiment(\"HyperOpt + Horovod on Spark + Mlflow\")\n",
    "argmin = fmin(\n",
    "    fn=train,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=16)\n",
    "print(\"Best value found: \", argmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb6701-943b-4ae8-86c7-fc0bbfa17559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_returnModel(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta(learning_rate)\n",
    "    loss = keras.losses.categorical_crossentropy\n",
    "    \n",
    "    \n",
    "    backend = SparkBackend(num_proc=set_num_proc,\n",
    "                       stdout=sys.stdout, stderr=sys.stderr,\n",
    "                       prefix_output_with_timestamp=True)\n",
    "    keras_estimator = hvd.KerasEstimator(backend=backend,\n",
    "                                         store=store,\n",
    "                                         model=model,\n",
    "                                         optimizer=optimizer,\n",
    "                                         loss=loss,\n",
    "                                         metrics=['accuracy'],\n",
    "                                         feature_cols=['features'],\n",
    "                                         label_cols=['label_vec'],\n",
    "                                         batch_size=set_batch_size,\n",
    "                                         epochs=set_epochs,\n",
    "                                         verbose=1)\n",
    "\n",
    "    keras_model = keras_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305336d-4580-4963-8bc4-ae02876bbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_mlflow = train_and_returnModel(argmin.get('learning_rate'))\n",
    "metadata = model_2_mlflow._get_metadata()\n",
    "floatx = model_2_mlflow._get_floatx()\n",
    "mlflow.keras.log_model(model_2_mlflow.getModel(), \"Keras-Sequential-model\",registered_model_name=\"Keras-Sequential-model-reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d3bdc-cd99-4097-b67e-8b5e44cb3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"models:/Keras-Sequential-model-reg/1\"\n",
    "loaded_model = mlflow.keras.load_model(model_uri)\n",
    "\n",
    "hvdKerasModel_from_mlfow = hvd.KerasModel(model=loaded_model,\n",
    "                                      feature_columns=['features'],\n",
    "                                      label_columns=['label_vec'],\n",
    "                                     _floatx = floatx,\n",
    "                                     _metadata = metadata)\n",
    "\n",
    "pred_df = hvdKerasModel_from_mlfow.transform(test_df)\n",
    "pred_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9770562-afc7-44b6-b7d0-53885c847958",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

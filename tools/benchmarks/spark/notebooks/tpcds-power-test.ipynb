{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b51606c-8fea-4c66-bf39-92b588243721",
   "metadata": {},
   "source": [
    "# Run TPC-DS power test with vanilla spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec202c-ea8f-47bf-9b54-a062cc492bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \"/home/cloudtik/runtime/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar:/home/cloudtik/runtime/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:/home/cloudtik/runtime/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar\")\n",
    "launcher.conf.set(\"spark.executor.extraClassPath\", \"/home/cloudtik/runtime/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar:/home/cloudtik/runtime/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:/home/cloudtik/runtime/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958bfab-f900-4f55-99ba-771a53351367",
   "metadata": {},
   "source": [
    "## Define the benchmark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb6963-c797-4d46-a6f0-ada565f122b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val scaleFactor = \"1000\"           // data scale 1GB\n",
    "val iterations = 1              // how many times to run the whole set of queries.\n",
    "val format = \"parquet\"          // support parquer or orc\n",
    "// support s3a://s3_bucket, gs://gs_bucket, hdfs://namenode_ip:9000\n",
    "// wasbs://container@storage_account.blob.core.windows.net\n",
    "// abfs://container@storage_account.dfs.core.windows.net\n",
    "val fsdir = \"hdfs://namenode_ip:9000\" \n",
    "val partitionTables = true      // create partition tables\n",
    "val query_filter = Seq()        // Seq() == all queries\n",
    "//val query_filter = Seq(\"q1-v2.4\", \"q2-v2.4\") // run subset of queries\n",
    "val randomizeQueries = false    // run queries in a random order. Recommended for parallel runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58549e-0f4e-4d9b-a193-5b45a82c6bfc",
   "metadata": {},
   "source": [
    "## Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c14699-5ea0-47c6-bf1f-e75e55827fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "// detailed results will be written as JSON to this location.\n",
    "var resultLocation = s\"${fsdir}/shared/data/results/tpcds_${format}/${scaleFactor}/\"\n",
    "var databaseName = s\"tpcds_${format}_scale_${scaleFactor}_db\"\n",
    "val use_arrow = false            // when you want to use gazella_plugin to run TPC-DS, you need to set it true.\n",
    "val data_path = s\"${fsdir}/shared/data/tpcds/tpcds_${format}/${scaleFactor}\"\n",
    "\n",
    "if (use_arrow){\n",
    "    val data_path= s\"${fsdir}/shared/data/tpcds/tpcds_${format}/${scaleFactor}\"\n",
    "    resultLocation = s\"${fsdir}/shared/data/results/tpcds_arrow/${scaleFactor}/\"\n",
    "    databaseName = s\"tpcds_arrow_scale_${scaleFactor}_db\"\n",
    "    val tables = Seq(\"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\", \"household_demographics\", \"income_band\", \"inventory\", \"item\", \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\", \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\", \"web_sales\", \"web_site\")\n",
    "    sql(s\"DROP database $databaseName CASCADE\")\n",
    "    if (spark.catalog.databaseExists(s\"$databaseName\")) {\n",
    "        println(s\"$databaseName has exists!\")\n",
    "    }else{\n",
    "        spark.sql(s\"create database if not exists $databaseName\").show\n",
    "        spark.sql(s\"use $databaseName\").show\n",
    "        for (table <- tables) {\n",
    "            if (spark.catalog.tableExists(s\"$table\")){\n",
    "                println(s\"$table has exists!\")\n",
    "            }else{\n",
    "                spark.catalog.createTable(s\"$table\", s\"$data_path/$table\", \"arrow\")\n",
    "            }\n",
    "        }\n",
    "        if (partitionTables) {\n",
    "            for (table <- tables) {\n",
    "                try{\n",
    "                    spark.sql(s\"ALTER TABLE $table RECOVER PARTITIONS\").show\n",
    "                }catch{\n",
    "                        case e: Exception => println(e)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}else{\n",
    "    if (spark.catalog.databaseExists(s\"$databaseName\")) {\n",
    "        println(s\"$databaseName has exists!\")\n",
    "    }else{\n",
    "        import com.databricks.spark.sql.perf.tpcds.TPCDSTables\n",
    "\n",
    "        println(s\"$databaseName doesn't exist. Creating...\")\n",
    "        val tables = new TPCDSTables(spark.sqlContext, \"\", s\"${scaleFactor}\", false)\n",
    "        tables.createExternalTables(data_path, format, databaseName, overwrite = true, discoverPartitions = partitionTables)\n",
    "    }\n",
    "}\n",
    "\n",
    "val timeout = 60 // timeout in hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d11c2c-8bd3-4b42-ac6c-10d9661a4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "// Spark configuration\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"10000\") // good idea for Q14, Q88.\n",
    "\n",
    "// ... + any other configuration tuning\n",
    "// COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb04ed-8ce2-4032-8d91-636cf0e025de",
   "metadata": {},
   "source": [
    "## Run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b9d44-2e4e-4e47-9f1c-a17ae6365d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql(s\"use $databaseName\")\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
    "val tpcds = new TPCDS (sqlContext = spark.sqlContext)\n",
    "def queries = {\n",
    "  val filtered_queries = query_filter match {\n",
    "    case Seq() => tpcds.tpcds2_4Queries\n",
    "    case _ => tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name))\n",
    "  }\n",
    "  if (randomizeQueries) scala.util.Random.shuffle(filtered_queries) else filtered_queries\n",
    "}\n",
    "val experiment = tpcds.runExperiment(\n",
    "  queries,\n",
    "  iterations = iterations,\n",
    "  resultLocation = resultLocation,\n",
    "  tags = Map(\"runtype\" -> \"benchmark\", \"database\" -> databaseName, \"scale_factor\" -> scaleFactor))\n",
    "\n",
    "println(experiment.toString)\n",
    "experiment.waitForFinish(timeout*60*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
